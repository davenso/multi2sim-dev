The page table walk
前面討論的mmu cache針對cpu,64bit  或者 gpu 32bit
還沒有針對64bit gpu的例子

要做的事：

1.研究multi2sim怎麼處理malloc 之類的system call

2.搞清楚m2s的處理機制,自動加上32bit -> 64bit的offset
  目前跑得workload,整隻程式程式allocate 500MB上下的memory

3.GPU也會對global的空間做存取
  模擬出gpu對memory存取的關係


論文實作：

模擬出gpu上面的的global memory access, 在64bit情況之下,的gpu mmu cache 設計


6/16
就page所定址能夠達到的範圍: page reach

gem5-gpu有提到的memory現象,在繼續做補充

直接signed extension 32bit memory address到64bit

6/17
page table在multi2im裡面是mentain在simulator的data stucture, 而不是在模擬出來的memory上面
可以嘗試加入event在memory指令上面（或許可以只處理gpu memory相關的指令？）
延遲access 4筆cache的指令之後,再往後繼續執行程式

1.先不支援unified virtual address, 只單純在gpu上面加上未來會實踐的virtual address轉physical address的機制
  先參照原本multi2sim的cpu, gpu模擬架構(2邊的memory會分開,自己看到自己的一塊)
  在gpu的memory指令那邊,多加上模擬mmu translation的機制
  MMU的cache也是加在gpu那邊
2.把translation cache的一些概念拿過來用
  需要先profile page walk的access pattern, 如果有重複性高的行為，則可以對translation做加速？？

6/18

multi2sim設定cpu是functional, gpu是timing
只考慮gpu那一邊的memory架構
在southern islandsouthern island上面  加上mmu translation的模擬

目標： custom memory system simulator
simulate mmu design on GPU



6/19
page table有無distance可以作壓縮減少空間？
個別level的tlb, 雖然已經有被做過了

也是一樣用統計tlb, page table access次數
TLB的的miss rate不嚴重的話大概是10％以內

計算average memory access latancy
將次數乘上lantancy反應回原本的時間



把x86建page table的code搬到gpu這邊
page table是一個一個擺到physical memory上面的


6/20
老師靠北的
1.     Show the current status of using Multi2sim to evaluate HSA

2.     Give your modeling of MMU, how we get trace, what results you have collected.

3.     Other slides you have info.

4.     Give benchmarks you have ported and how you ported, what routines you modified

x86模擬使用address translation的時機
multi2sim用自己實現的x86 micro instruction
所以在memory指令的前面,會有會有計算address的effective指令
此address會存在context,下一筆load/store會直接用這個effective adress作存取

在emu的時候的fetch階段

6/21
GPU上面的global address是怎嚜計算得來的？
在x86是要在前面有一個effa指令計算

LDS的memory range
x86等待memory指令finish是如何實現？



6/23
witness ptr在mod_access完成的時候會被設定為1
多建立2個stage作stage的translation,順便紀錄latency
要有類似x86的page機制,get_page那邊去重新allocate gpu 的global memory
$$$$$$$$s
切某一段physical memory address專門放page table, 可以在上面實現tlb或者其他類型的cache
可以先跑出baseline的gpu mmu design的數據
global_mem_witness會跑過全部的wavefromt, southern island 7970總共64個所以在si_vector_mem_mem stage會將witness變成-64
下一個stage會等待全部的memory access都都處理好, witness ptr被累加回0之後才進行.

gpu mmu 架構裡面要有個cr3 register

6/28
graph algorithm suffur much more on TLB miss


6/29
把configuration設定的跟kaveri接近
1. How to build up the gpu virtual address support and mmu simulation environment
2. Run baseline SRAM cache and get basic performance
%  Problem is the lack of multiple access of memory
3. Use edram to help imrpove parallel and density
%  Compare different row bank configuration

4. open / close problem
%  The overhead of edram, because the high miss rate of each benchmark
%  consider


5. Run opencl program on Kaveri, compare the baseline design runtime between kaveri and multi2sim (verification)





有了這個架構或許可以同時進行2個gpu process的mmu架構模擬
同時2個gpu在跑的memory會有什麼問題？

方法,觀察現象：
1.觀察page table執行路徑
  用functional model去分析所有的路徑機率

2.觀察page table重複走的情況
  timing model分析path被重複走到的情況
2.page 壓縮
  算entropy, egergodic

3.分析benchmark 為什麼有的好有的不好






修改host machine的kernel code
把opencl程式跑在cpu上面
得到global address的VA->PA轉換關係的page table
dump出來變成multi2sim的config檔
multi2sim讀取這個config檔
設定mmu內部的記憶體映射關係
可能用memory module access去設定之類的



可能問題在於benchmark在compile階段
是用demand paging的方式做memory的allocation



工作1:  得到實體機器上面

工作2:  要用m2c提供的提供的flow去編譯才不會在llvm2si的時候core dump, 可以改掉不支援struct的格式,但是有用到math.h的話就還是無解


已經porting好多個rodinia benchmark
跑一下一下tlb miss看看miss rate是否跟gem5-gpu的數據一致

6/15

kernel function編譯目前只能夠使用call amd driver, 下-O0和調整其他的benchmark code

memory trace的部份在translation cache有提到他們的作法, 也是也是只有取virtual address的trace,至於對應的physical mapping沒有特別提到


